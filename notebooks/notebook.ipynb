{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T18:16:29.723351Z",
     "start_time": "2020-04-08T18:16:06.073407Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T18:16:30.260442Z",
     "start_time": "2020-04-08T18:16:29.727265Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T18:16:30.395257Z",
     "start_time": "2020-04-08T18:16:30.260442Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Training data shape:  (27486, 4)\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 27486 entries, 0 to 27485\nData columns (total 4 columns):\n #   Column         Non-Null Count  Dtype \n---  ------         --------------  ----- \n 0   textID         27486 non-null  object\n 1   text           27485 non-null  object\n 2   selected_text  27485 non-null  object\n 3   sentiment      27486 non-null  object\ndtypes: object(4)\nmemory usage: 429.5+ KB\nTesting data shape:  (3535, 3)\n"
    }
   ],
   "source": [
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv')\n",
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv')\n",
    "\n",
    "print('Training data shape: ', train.shape)\n",
    "train.info()\n",
    "print('Testing data shape: ', test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric\n",
    "\n",
    "Higher the score, the more similar the two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T18:16:30.406557Z",
     "start_time": "2020-04-08T18:16:30.397167Z"
    }
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    '''\n",
    "    Intersection of words in the sentence / Union of all words\n",
    "    '''\n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "str1 = \"Let us compare the two strings\"\n",
    "str2 = \"Let us compare the two strings\"\n",
    "str3 = \"Let us compare some random strings\"\n",
    "\n",
    "print(jaccard(str1, str2))\n",
    "\n",
    "print(jaccard(str1, str3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T18:16:30.420844Z",
     "start_time": "2020-04-08T18:16:30.408528Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "textID           0\ntext             1\nselected_text    1\nsentiment        0\ndtype: int64 \n\ntextID       0\ntext         0\nsentiment    0\ndtype: int64\n"
    }
   ],
   "source": [
    "print(train.isnull().sum(),'\\n')\n",
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T18:16:30.441825Z",
     "start_time": "2020-04-08T18:16:30.423600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropping missing values in training\n",
    "train.dropna(axis=0, how=\"any\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Head values of all the sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T18:16:30.472738Z",
     "start_time": "2020-04-08T18:16:30.443925Z"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "~~~~~~~~~positive~~~~~~~~\n1        Oh! Good idea about putting them on ice cream\n4               haha better drunken tweeting you mean?\n6    had an awsome salad! I recommend getting the S...\n7     fine! Going to do my big walk today 20 or so ...\n8          Thank a yoou  how are you? #TwitterTakeover\nName: text, dtype: object\n\n~~~~~~~~~negative~~~~~~~~\n3            i dont think you can vote anymore! i tried\n5                          headache  wanna see my Julie\n12                                     Miss you my dear\n15    Today Dan bought me Bio Dome AND the Reality B...\n16                                oo noo thats not good\nName: text, dtype: object\n\n~~~~~~~~~neutral~~~~~~~~~\n0     Spent the entire morning in a meeting w/ a ven...\n2     says good (or should i say bad?) afternoon!  h...\n9     Why don't adobe realise no one WANTS to pay fo...\n10                      PRD take a long time to review!\n11    _2008 Well, having to revise them!  Was to do ...\nName: text, dtype: object\n\n"
    }
   ],
   "source": [
    "for sentiment in [\"positive\", \"negative\", \"neutral\"] :\n",
    "    print(sentiment.center(25,'~'))\n",
    "    print(train.query(f'sentiment == \"{sentiment}\"')['text'].head())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T18:16:30.501901Z",
     "start_time": "2020-04-08T18:16:30.478010Z"
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "neutral     40.447517\npositive    31.224304\nnegative    28.328179\nName: sentiment, dtype: float64"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "train.sentiment.value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text data preprocessing\n",
    "\n",
    "1. Make text lowercase\n",
    "- removes hyperlinks\n",
    "- remove punctuation\n",
    "- removes numbers\n",
    "- tokenizes\n",
    "- removes stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T18:16:30.518750Z",
     "start_time": "2020-04-08T18:16:30.505152Z"
    }
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    # Hyperlinks / Text in \n",
    "    text = re.sub()\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    \n",
    "    # Punctuations\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    # re.escape makes sure special characters are escaped\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    \n",
    "    # Remove new line\n",
    "    text = re.sub('\\n', '', text)\n",
    "    # Removes alpha numeric and numbers in the text\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    \"\"\"\n",
    "        Cleaning and parsing the text.\n",
    "    \"\"\"\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    nopunc = clean_text(text)\n",
    "    tokenized_text = tokenizer.tokenize(nopunc)\n",
    "    #remove_stopwords = [w for w in tokenized_text if w not in stopwords.words('english')]\n",
    "    combined_text = ' '.join(tokenized_text)\n",
    "    return combined_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Text Statistics\n",
    "We can now do some statistical analysis to explore the fundamental characteristics of the text data. Some of the analysis which can be useful are:\n",
    "\n",
    "1. Text length analysis\n",
    "2. Word frequency analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-08T18:26:04.868885Z",
     "start_time": "2020-04-08T18:26:04.686138Z"
    }
   },
   "outputs": [],
   "source": [
    "train['tweet_length'] = train['text'].apply(len)\n",
    "train['word_frequency'] = train['text'].str.split().apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "       tweet_length  word_frequency\ncount  27485.000000    27485.000000\nmean      68.727779       12.903693\nstd       35.963657        6.926507\nmin        3.000000        1.000000\n25%       39.000000        7.000000\n50%       64.000000       12.000000\n75%       97.000000       18.000000\nmax      165.000000       33.000000",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet_length</th>\n      <th>word_frequency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>27485.000000</td>\n      <td>27485.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>68.727779</td>\n      <td>12.903693</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>35.963657</td>\n      <td>6.926507</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>3.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>39.000000</td>\n      <td>7.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>64.000000</td>\n      <td>12.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>97.000000</td>\n      <td>18.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>165.000000</td>\n      <td>33.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# train.loc[:, ['tweet_length', 'word_frequency']].describe()\n",
    "\n",
    "train[['tweet_length', 'word_frequency']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the sentiment terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://www.kaggle.com/parulpandey/basic-preprocessing-and-eda\n",
    "2. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}